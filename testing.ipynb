{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c4c2c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from textblob import TextBlob\n",
    "import replicate\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83f4788d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Problem Statement</th>\n",
       "      <th>User Input</th>\n",
       "      <th>Expected Sentiment</th>\n",
       "      <th>Emotion Context</th>\n",
       "      <th>Expected Response</th>\n",
       "      <th>Step in Conversation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7/4 * 8/5</td>\n",
       "      <td>Can you help me start this problem?</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Unsure</td>\n",
       "      <td>Let's go through it step-by-step.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7/4 * 8/5</td>\n",
       "      <td>Is my answer correct:</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Checking</td>\n",
       "      <td>Let's go through it step-by-step.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7/4 * 8/5</td>\n",
       "      <td>This makes no sense to me at all!</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Frustrated</td>\n",
       "      <td>Don't worry, let's figure it out together.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7/4 * 8/5</td>\n",
       "      <td>I'm not sure why we need to find a common deno...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Curious</td>\n",
       "      <td>Let's go through it step-by-step.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7/4 * 8/5</td>\n",
       "      <td>Here's what I tried:</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Explaining</td>\n",
       "      <td>Let's go through it step-by-step.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Problem Statement                                         User Input  \\\n",
       "0         7/4 * 8/5                Can you help me start this problem?   \n",
       "1         7/4 * 8/5                             Is my answer correct:    \n",
       "2         7/4 * 8/5                  This makes no sense to me at all!   \n",
       "3         7/4 * 8/5  I'm not sure why we need to find a common deno...   \n",
       "4         7/4 * 8/5                              Here's what I tried:    \n",
       "\n",
       "  Expected Sentiment Emotion Context  \\\n",
       "0            Neutral          Unsure   \n",
       "1            Neutral        Checking   \n",
       "2           Negative      Frustrated   \n",
       "3            Neutral         Curious   \n",
       "4            Neutral      Explaining   \n",
       "\n",
       "                            Expected Response  Step in Conversation  \n",
       "0           Let's go through it step-by-step.                     2  \n",
       "1           Let's go through it step-by-step.                     3  \n",
       "2  Don't worry, let's figure it out together.                     3  \n",
       "3           Let's go through it step-by-step.                     3  \n",
       "4           Let's go through it step-by-step.                     3  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_dataset = pd.read_csv(\"test_dataset.csv\")\n",
    "\n",
    "chatbot_dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dd2483c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"REPLICATE_API_TOKEN\"] = \"r8_VTIlJKo4ybjVLEMSa2BCg7HiYNOK9UA1BtOsc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d9dc203",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Chinu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure NLTK components are downloaded\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40945476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Helper function to simulate chatbot response using the deployed chatbot\n",
    "# def simulate_chatbot_response(user_input):\n",
    "#     # Use Replicate API to get the chatbot's response\n",
    "#     model = replicate.models.get(\"meta/llama-2-7b-chat\")\n",
    "#     version = model.versions.get(\"latest\")\n",
    "#     response = version.predict(prompt=user_input)\n",
    "#     return response[\"generated_text\"] if \"generated_text\" in response else \"Error in response\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b07a87c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def simulate_chatbot_response(user_input):\n",
    "#     try:\n",
    "#         # Fetch the model and version\n",
    "#         model = replicate.models.get(\"meta/llama-2-7b-chat\")\n",
    "#         version = model.versions.get(\"latest\")\n",
    "        \n",
    "#         # Generate response\n",
    "#         response = version.predict(prompt=user_input)\n",
    "#         return response.get(\"generated_text\", \"Error: No generated_text found in response\")\n",
    "#     except replicate.exceptions.ReplicateError as e:\n",
    "#         return f\"ReplicateError occurred: {e}\"\n",
    "#     except Exception as e:\n",
    "#         return f\"An unexpected error occurred: {e}\"\n",
    "\n",
    "\n",
    "def simulate_chatbot_response(user_input):\n",
    "    \"\"\"\n",
    "    Simulate the chatbot's response using the LLAMA 2 model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response_text = \"\"\n",
    "        for event in replicate.stream(\n",
    "            \"meta/llama-2-7b-chat\",\n",
    "            input={\"prompt\": user_input},  # Adjust input key if required\n",
    "        ):\n",
    "            response_text += str(event)\n",
    "        return response_text.strip()\n",
    "    except replicate.exceptions.ReplicateError as e:\n",
    "        return f\"ReplicateError occurred: {e}\"\n",
    "    except Exception as e:\n",
    "        return f\"An unexpected error occurred: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c531c6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e27208ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics storage\n",
    "bleu_scores = []\n",
    "rouge_scores = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []}\n",
    "sentiment_matches = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5937e0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_chatbot(dataset):\n",
    "    global sentiment_matches\n",
    "    for index, row in dataset.iterrows():\n",
    "        user_input = row['User Input']\n",
    "        expected_response = row['Expected Response']\n",
    "        expected_sentiment = row['Expected Sentiment']\n",
    "#         print(f\"User Input: {user_input}\")\n",
    "        \n",
    "#         print(f\"Expected Response: {expected_response}\")\n",
    "\n",
    "\n",
    "        # Simulate chatbot response\n",
    "        chatbot_response = simulate_chatbot_response(user_input)\n",
    "#         print(f\"Chatbot Response: {chatbot_response}\")\n",
    "\n",
    "        # Calculate BLEU score\n",
    "        reference = nltk.word_tokenize(expected_response.lower())\n",
    "        candidate = nltk.word_tokenize(chatbot_response.lower())\n",
    "        bleu = sentence_bleu([reference], candidate)\n",
    "        bleu_scores.append(bleu)\n",
    "\n",
    "        # Calculate ROUGE scores\n",
    "        rouge = scorer.score(expected_response, chatbot_response)\n",
    "        for key in rouge_scores:\n",
    "            rouge_scores[key].append(rouge[key].fmeasure)\n",
    "\n",
    "        # Sentiment analysis\n",
    "        detected_sentiment = \"Positive\" if TextBlob(chatbot_response).sentiment.polarity > 0 else \"Negative\"\n",
    "        if detected_sentiment == expected_sentiment:\n",
    "            sentiment_matches += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6905573",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = chatbot_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce98e64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "evaluate_chatbot(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c34db57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results:\n",
      "Average BLEU Score: 5.660657563081191e-81\n",
      "Average ROUGE Scores:\n",
      "  rouge1: 0.027206853231349897\n",
      "  rouge2: 0.0021046246717091525\n",
      "  rougeL: 0.022797902749095503\n",
      "Sentiment Alignment Accuracy: 6.583333333333333%\n"
     ]
    }
   ],
   "source": [
    "# Summarize results\n",
    "average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "average_rouge = {key: sum(values) / len(values) for key, values in rouge_scores.items()}\n",
    "sentiment_accuracy = (sentiment_matches / len(chatbot_dataset)) * 100\n",
    "\n",
    "# Print results\n",
    "print(\"Evaluation Results:\")\n",
    "print(f\"Average BLEU Score: {average_bleu}\")\n",
    "print(\"Average ROUGE Scores:\")\n",
    "for key, value in average_rouge.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(f\"Sentiment Alignment Accuracy: {sentiment_accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f6ff19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment with different ways of phrasing the input prompts to improve the chatbot's responses. \n",
    "#This approach leverages natural language's flexibility to guide the model's outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e410988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_prompts(dataset, prompt_variations):\n",
    "#     \"\"\"\n",
    "#     Evaluate different prompt variations against the test dataset.\n",
    "#     \"\"\"\n",
    "#     results = {}\n",
    "    \n",
    "#     for prompt_template in prompt_variations:\n",
    "#         bleu_scores = []\n",
    "#         rouge_scores = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []}\n",
    "#         sentiment_matches = 0\n",
    "        \n",
    "#         for index, row in dataset.iterrows():\n",
    "#             user_input = row[\"User Input\"]\n",
    "#             expected_response = row[\"Expected Response\"]\n",
    "#             expected_sentiment = row[\"Expected Sentiment\"]\n",
    "            \n",
    "#             # Apply prompt template\n",
    "#             prompt = prompt_template.format(user_input=user_input)\n",
    "            \n",
    "#             # Simulate chatbot response\n",
    "#             chatbot_response = simulate_chatbot_response(prompt)\n",
    "            \n",
    "#             # BLEU score\n",
    "#             reference = nltk.word_tokenize(expected_response.lower())\n",
    "#             candidate = nltk.word_tokenize(chatbot_response.lower())\n",
    "#             bleu = sentence_bleu([reference], candidate)\n",
    "#             bleu_scores.append(bleu)\n",
    "            \n",
    "#             # ROUGE scores\n",
    "#             rouge = scorer.score(expected_response, chatbot_response)\n",
    "#             for key in rouge_scores:\n",
    "#                 rouge_scores[key].append(rouge[key].fmeasure)\n",
    "            \n",
    "#             # Sentiment alignment\n",
    "#             detected_sentiment = (\n",
    "#                 \"Positive\" if TextBlob(chatbot_response).sentiment.polarity > 0 else \"Negative\"\n",
    "#             )\n",
    "#             if detected_sentiment == expected_sentiment:\n",
    "#                 sentiment_matches += 1\n",
    "        \n",
    "#         # Store results for this prompt\n",
    "#         results[prompt_template] = {\n",
    "#             \"Average BLEU\": sum(bleu_scores) / len(bleu_scores),\n",
    "#             \"Average ROUGE\": {key: sum(scores) / len(scores) for key, scores in rouge_scores.items()},\n",
    "#             \"Sentiment Accuracy\": sentiment_matches / len(dataset) * 100,\n",
    "#         }\n",
    "    \n",
    "#     return results\n",
    "\n",
    "# # Define prompt variations\n",
    "# prompt_variations = [\n",
    "#     \"{user_input}\",\n",
    "#     \"Explain step-by-step: {user_input}\",\n",
    "#     \"I struggle with this topic: {user_input}. Can you help?\",\n",
    "#     \"Help me reduce my anxiety and understand: {user_input}\",\n",
    "# ]\n",
    "\n",
    "# # Example usage\n",
    "# results = evaluate_prompts(chatbot_dataset, prompt_variations)\n",
    "# print(\"Prompt Engineering Results:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1851107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template: Explain step-by-step how to handle this: {user_input}\n",
      "  Average BLEU: 2.8521174323139885e-156\n",
      "  Average ROUGE-1: 0.03496420008801025\n",
      "  Average ROUGE-2: 0.00523020237789709\n",
      "  Average ROUGE-L: 0.02604780761728564\n",
      "  Sentiment Accuracy: 10.0\n",
      "Template: I feel nervous about {user_input}. Can you help?\n",
      "  Average BLEU: 3.864939236718779e-156\n",
      "  Average ROUGE-1: 0.03846004314325603\n",
      "  Average ROUGE-2: 0.0031747799933162526\n",
      "  Average ROUGE-L: 0.029041189765154247\n",
      "  Sentiment Accuracy: 10.0\n",
      "Template: Please provide a simple explanation for: {user_input}\n",
      "  Average BLEU: 1.6916991823740183e-156\n",
      "  Average ROUGE-1: 0.0439910720800359\n",
      "  Average ROUGE-2: 0.001351351351351351\n",
      "  Average ROUGE-L: 0.029925088166378584\n",
      "  Sentiment Accuracy: 10.0\n",
      "Template: How do I solve {user_input}? Give an easy method.\n",
      "  Average BLEU: 2.809547325219266e-156\n",
      "  Average ROUGE-1: 0.03122744171629354\n",
      "  Average ROUGE-2: 0.003058883604658253\n",
      "  Average ROUGE-L: 0.021046222594307797\n",
      "  Sentiment Accuracy: 10.0\n"
     ]
    }
   ],
   "source": [
    "def evaluate_prompts(dataset, prompt_variations):\n",
    "    \"\"\"\n",
    "    Evaluate different prompt templates and compare their performance.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for prompt_template in prompt_variations:\n",
    "        bleu_scores = []\n",
    "        rouge_scores = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []}\n",
    "        sentiment_matches = 0\n",
    "        \n",
    "        for _, row in dataset.iterrows():\n",
    "            user_input = row[\"User Input\"]\n",
    "            expected_response = row[\"Expected Response\"]\n",
    "            expected_sentiment = row[\"Expected Sentiment\"]\n",
    "            \n",
    "            # Apply prompt template\n",
    "            prompt = prompt_template.format(user_input=user_input)\n",
    "            \n",
    "            # Get chatbot response\n",
    "            chatbot_response = simulate_chatbot_response(prompt)\n",
    "#             print(f\"Prompt: {prompt} => Response: {chatbot_response}\")\n",
    "            \n",
    "            # BLEU score\n",
    "            reference = nltk.word_tokenize(expected_response.lower())\n",
    "            candidate = nltk.word_tokenize(chatbot_response.lower())\n",
    "            bleu = sentence_bleu([reference], candidate)\n",
    "            bleu_scores.append(bleu)\n",
    "            \n",
    "            # ROUGE scores\n",
    "            rouge = scorer.score(expected_response, chatbot_response)\n",
    "            for key in rouge_scores:\n",
    "                rouge_scores[key].append(rouge[key].fmeasure)\n",
    "            \n",
    "            # Sentiment alignment\n",
    "            detected_sentiment = (\n",
    "                \"Positive\" if TextBlob(chatbot_response).sentiment.polarity > 0 else \"Negative\"\n",
    "            )\n",
    "            if detected_sentiment == expected_sentiment:\n",
    "                sentiment_matches += 1\n",
    "\n",
    "        # Store average metrics for the prompt template\n",
    "        results[prompt_template] = {\n",
    "            \"Average BLEU\": sum(bleu_scores) / len(bleu_scores),\n",
    "            \"Average ROUGE-1\": sum(rouge_scores[\"rouge1\"]) / len(rouge_scores[\"rouge1\"]),\n",
    "            \"Average ROUGE-2\": sum(rouge_scores[\"rouge2\"]) / len(rouge_scores[\"rouge2\"]),\n",
    "            \"Average ROUGE-L\": sum(rouge_scores[\"rougeL\"]) / len(rouge_scores[\"rougeL\"]),\n",
    "            \"Sentiment Accuracy\": (sentiment_matches / len(dataset)) * 100,\n",
    "        }\n",
    "    return results\n",
    "\n",
    "\n",
    "prompt_variations = [\n",
    "    \"Explain step-by-step how to handle this: {user_input}\",\n",
    "    \"I feel nervous about {user_input}. Can you help?\",\n",
    "    \"Please provide a simple explanation for: {user_input}\",\n",
    "    \"How do I solve {user_input}? Give an easy method.\",\n",
    "]\n",
    "\n",
    "# Evaluate prompts\n",
    "prompt_results = evaluate_prompts(sample_data, prompt_variations)\n",
    "for template, metrics in prompt_results.items():\n",
    "    print(f\"Template: {template}\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b763884",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment Analysis Variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2af5b902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with Sentiment Analysis: {'Average BLEU': 6.867738152376117e-232, 'Average ROUGE': {'rouge1': 0.022627504055925278, 'rouge2': 0.0, 'rougeL': 0.01675928168118266}, 'Sentiment Accuracy': 10.0}\n",
      "Results without Sentiment Analysis: {'Average BLEU': 1.7552116553749679e-156, 'Average ROUGE': {'rouge1': 0.02246129729009667, 'rouge2': 0.0013422818791946306, 'rougeL': 0.01725748363401081}, 'Sentiment Accuracy': 'N/A'}\n"
     ]
    }
   ],
   "source": [
    "def evaluate_with_without_sentiment(dataset, use_sentiment=True):\n",
    "    \"\"\"\n",
    "    Evaluate the chatbot with and without sentiment analysis.\n",
    "    \"\"\"\n",
    "    bleu_scores = []\n",
    "    rouge_scores = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []}\n",
    "    sentiment_matches = 0\n",
    "    \n",
    "    for index, row in dataset.iterrows():\n",
    "        user_input = row[\"User Input\"]\n",
    "        expected_response = row[\"Expected Response\"]\n",
    "        expected_sentiment = row[\"Expected Sentiment\"]\n",
    "        \n",
    "        # Simulate chatbot response\n",
    "        chatbot_response = simulate_chatbot_response(user_input)\n",
    "        \n",
    "        # BLEU score\n",
    "        reference = nltk.word_tokenize(expected_response.lower())\n",
    "        candidate = nltk.word_tokenize(chatbot_response.lower())\n",
    "        bleu = sentence_bleu([reference], candidate)\n",
    "        bleu_scores.append(bleu)\n",
    "        \n",
    "        # ROUGE scores\n",
    "        rouge = scorer.score(expected_response, chatbot_response)\n",
    "        for key in rouge_scores:\n",
    "            rouge_scores[key].append(rouge[key].fmeasure)\n",
    "        \n",
    "        if use_sentiment:\n",
    "            # Sentiment alignment\n",
    "            detected_sentiment = (\n",
    "                \"Positive\" if TextBlob(chatbot_response).sentiment.polarity > 0 else \"Negative\"\n",
    "            )\n",
    "            if detected_sentiment == expected_sentiment:\n",
    "                sentiment_matches += 1\n",
    "    \n",
    "    return {\n",
    "        \"Average BLEU\": sum(bleu_scores) / len(bleu_scores),\n",
    "        \"Average ROUGE\": {key: sum(scores) / len(scores) for key, scores in rouge_scores.items()},\n",
    "        \"Sentiment Accuracy\": sentiment_matches / len(dataset) * 100 if use_sentiment else \"N/A\",\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "results_with_sentiment = evaluate_with_without_sentiment(sample_data, use_sentiment=True)\n",
    "results_without_sentiment = evaluate_with_without_sentiment(sample_data, use_sentiment=False)\n",
    "\n",
    "print(\"Results with Sentiment Analysis:\", results_with_sentiment)\n",
    "print(\"Results without Sentiment Analysis:\", results_without_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8142a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
